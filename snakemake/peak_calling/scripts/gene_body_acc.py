"""
Author: Pablo Mendieta 
Date: 2021-06-04

The purpose of this script is to calculate gene body accessability for single
cell ATAC-seq data. The expected inputs here are two bed files. One bed file
containining the genome annotations of all protein coding gene, and the other
featuring the 1bp intergration sites generated by the scATAC seq data.

This script utilizes pybedtools heavily https://github.com/daler/pybedtools

Things to possible change:

THis script was originally written to work on Thaliana. So we're only taking
500 bp up and downstream of each gene. If working on a larger genome this will
likely need to be increased. 


Downstream this script is supposed to be read into merge_gene_activity.R with
the cicero output as well to get our finalized gene accessability metric to be
fed into our marker ID scripts. 

"""

import argparse
import sys
import os
import pybedtools
import subprocess
import random 
import string




def read_bed_file(bed_file):
    """Takes in bed file and reads it in using pybedtools. Will make life
    easier longitudinally.

    :bed_file: TODO
    :returns: TODO

    """
    

    try:
        os.path.isfile(bed_file)
    except:
        FileNotFoundError

    bed_file_load = pybedtools.BedTool(bed_file)
    return bed_file_load


def remove_if_exists(filename):
    """Remove the file if it exists.
    :returns: TODO
    """
    try:
        os.remove(filename)
    except OSError:
        pass

def keep_nfeatures(feature, n):
    """Given a list of features, keep only-
    n number of fields

    """
    new_feature = feature[:int(n)]
    feature = new_feature

    return feature


def keep_final_set(feature):
    """Keep only the bed features we care about

    """
    good_indicies = [0,1,2,3,4,8,9]
    new_feature =  [feature[i] for i in good_indicies]
    feature = new_feature
    return feature



def expand_ft(feature, n):
    """Given a list of features, expand them by N bp up and downstream.
    Strandedness does not matter.
    """
    new_start = int(feature.start) - n
    new_stop = int(feature.stop) + n

    feature.start = new_start 
    feature.stop = new_stop

    return feature



def modify_associated_score(feature, tn5_dict):
    """Given a list of features, expand them by N bp up and downstream.
    Strandedness does not matter.
    """
    take_associated_score = feature[-1]

    take_cell_ID = feature[4]

    grab_tn5_from_dict = tn5_dict[take_cell_ID]

    
    
    final_val = int(take_associated_score)/grab_tn5_from_dict

    #print(feature)
    #print(take_associated_score)
    #print(grab_tn5_from_dict)
    #print("Final Calculate Val")
    #print(final_val)
    
    feature[-1] = str(final_val)
    #print(feature)

    return feature

def calc_num_reads(tn5_bed):
    """TODO: Docstring for calc_num_reads.

    :arg1: TODO
    :returns: TODO

    """
    cell_ID_read_counts = {}

    for read in tn5_bed:
        print(read)
        input()
        if read[9] not in cell_ID_read_counts:
            cell_ID_read_counts[read[9]] = 1
        elif read[9] in cell_ID_read_counts:
            cell_ID_read_counts[read[9]] += 1
    return(cell_ID_read_counts)


def get_parser():
    parser = argparse.ArgumentParser(description='Calculate regions with fold \
            enrichment over X and outputs these regions.')
    parser.add_argument('-anno', "--annotation_bed_file", help="Annotation file \
    comprise of only gene features", required=True, dest='anno')
    parser.add_argument('-tn5', "--tn5_bed", help="Tn5 Intergration bed file", 
            required=True, dest='tn5'),
    parser.add_argument('-base', "--base_name", help="Tn5 Intergration bed file", 
        required=True, dest='base'),
    parser.add_argument('-normalize', "--normalize", help="Tn5 Intergration bed file", 
        required=False, dest='norm')
    parser.add_argument('-p', "--threads", help="Number of threads to run with sort ", 
        required=False, dest='thread')
    parser.add_argument('-type', "--type", help="Genes or ACRs? Note genes will \
            be extended by 500 BPs in either direction and ACRs will NOT.", 
        required=False, dest='type')

    parser.add_argument('-o', "--output", help=" Output file. If not \
    given will be print ", required=False, dest='o')
    args = vars(parser.parse_args())    
    return parser


if __name__ == "__main__":
    args = get_parser().parse_args()

    gen_thread_string = ""

    if args.thread != None and type(int(args.thread)) == int:
        gen_num_threads =  " --parallel=" + str(args.thread) + " "
    else:
        gen_num_threads = ""


    sys.stderr.write("Loading Annotation\n")
    annot_file = read_bed_file(args.anno)
    annot_cut = annot_file.each(keep_nfeatures, 5).saveas()
    
    if args.type != None:
        if "ACR" in args.type.upper():
            print("Working on accessability of ACRs. Not extended")
            annot_expand = annot_cut.remove_invalid().saveas()
        elif "GENE" in args.type.upper():
            print("Working on accessability of Genes. Extended by 500 BPs")
            annot_expand = annot_cut.each(expand_ft, 500).remove_invalid()
            print("Annotation Passing moving On")
    elif args.type == None:
        print("No Type Given - No Extension added")
        annot_expand = annot_cut.remove_invalid().saveas()





    sys.stderr.write("Loading Tn5 Intergration\n")
    tn5_file = read_bed_file(args.tn5)
    
     

    sys.stderr.write("Intersecting tn5 and Gene Annotations\n")
    annot_intersect_tn5 = annot_expand.intersect(tn5_file, wa = True, 
            wb = True, sorted = True).saveas()

    sys.stderr.write("Counting Occuerences\n")

    #letters = string.ascii_lowercase
    # = ''.join(random.choice(letters) for i in range(10)) + ".bed"


    bed_base = args.base + ".bed"
    bed_base_sorted = args.base + "sorted.bed"

    if args.norm != None:

        sys.stderr.write("Calculating Normalization\n")
        cell_ID_tn5_ct = calc_num_reads(annot_intersect_tn5)
        
        

        final_set = annot_intersect_tn5.each(keep_final_set).saveas(bed_base)

        sys.stderr.write("Sorting - this might take time  \n")
        command_gen = "sort -k1,1 -k2,2 -k3,3 -k4,4 -k5,5 -T ." + gen_num_threads + bed_base + " > " + bed_base_sorted
 
  
        try:
            subprocess.call(command_gen, shell=True)
        except OSError:
            print("This command had to work for this to run")
            sys.exit(-2)

        final_set = read_bed_file(bed_base_sorted)
        count_annot_tn5_intersect = final_set.groupby(g=[1,2,3,4,5],c=6, o=['count']).saveas()

        final_gene_body_score = count_annot_tn5_intersect.each(modify_associated_score, cell_ID_tn5_ct)

    elif args.norm == None:

        sys.stderr.write("No Normalizaton \n")
        final_set = annot_intersect_tn5.each(keep_final_set).saveas(bed_base)

        sys.stderr.write("Sorting - this might take time  \n")
        command_gen = "sort -k1,1 -k2,2 -k3,3 -k4,4 -k5,5 -T ." + gen_num_threads + bed_base + " > " + bed_base_sorted


        try:
            subprocess.call(command_gen, shell=True)
        except OSError:
            print("This command had to work for this to run")
            sys.exit(-2)
        final_set = read_bed_file(bed_base_sorted)
        final_gene_body_score = final_set.groupby(g=[1,2,3,4,5,6],c=7, o=['count']).saveas()

    else:
        print("ERROR. Normalization must be given")
        sys.error(-2)

    
    sys.stderr.write("Writing to Output\n")
    if args.o != None:
        remove_if_exists(args.o)
        with open(args.o, 'a+') as f:
            for read in final_gene_body_score:
                final = [read[3], read[5], str(read[6])]
                f.write('\t'.join(final))
                f.write('\n')

    elif args.o == None:
        for read in final_gene_body_score:
            #print(read)
            final_string = "\t".join([read[3], read[5], str(read[6])])
            print(final_string)
    

    
    #total_sum = {}
    #for read in final_gene_body_score:
    #    if read[4] not in total_sum:
    #        total_sum[read[4]] = [float(read[5])]

    #    elif read[4] in total_sum:
    #        total_sum[read[4]].append(float(read[5]))

    #for key, val in total_sum.items():
    #    print(key, sum(val))

